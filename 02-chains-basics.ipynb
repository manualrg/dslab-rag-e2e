{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f10199-9bb5-488b-99ad-f43a16f71205",
   "metadata": {},
   "source": [
    "# LCEL and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8ae065-59d9-473a-a3ad-cef76ff962cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f434c7-b90c-4d59-aa61-a7c19158dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_mistralai import  ChatMistralAI\n",
    "from src import utils, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d368b0",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f55ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_settings = conf.load(file=\"settings.yaml\")\n",
    "conf_settings\n",
    "\n",
    "LLM_WORKHORSE = conf_settings.llm_workhorse\n",
    "LLM_FLAGSHIP = conf_settings.llm_flagship\n",
    "EMBEDDINGS = conf_settings.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18ae9c",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766b9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "MISTRAL_API_KEY = os.environ[\"MISTRAL_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae64766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-5-mini-2025-08-07'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_WORKHORSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce456f-9041-4190-9ff6-6b4df407e747",
   "metadata": {},
   "source": [
    "# What is a Langchain chain\n",
    "\n",
    "It is a composition element that allow to build an structured pipeline to perform IA Generative tasks, specially (but not only) for RAGs\n",
    "\n",
    "\n",
    "LAngchain chains are built (in version 1.x or above) using LCEL (LangChain Expression Language)\n",
    "\n",
    "Its core principles are: composability, streaming, async, parallelism\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2854e",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4148496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you tell me the distance from the Earth to the Moon?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_chat_hist = [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{question}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt.invoke(\"Can you tell me the distance from the Earth to the Moon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "168e82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "prompt.format_prompt(topic=\"Devops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4278186a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"topic\": \"Devops\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950198a",
   "metadata": {},
   "source": [
    "## FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf267e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define examples\n",
    "examples = [  # input/output keys\n",
    "    {\"input\": \"Q: What is LangChain?\", \"output\": \"A: LangChain is a framework for building applications powered by large language models (LLMs).\"},\n",
    "    {\"input\": \"Q: What is LCEL?\", \"output\": \"A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.\"},\n",
    "]\n",
    "\n",
    "# 2. Create an example prompt template\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "# 3. Few-shot wrapper\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "# 4. Final prompt template (instructions + few-shots + new user question)\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise AI assistant. Answer clearly.\\\n",
    "     The answer style should be like the following examples:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae4a200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt.invoke(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d836514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LCEL?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(few_shot_prompt\n",
    "          .format_prompt() \n",
    "          .to_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae6f6ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a concise AI assistant. Answer clearly.     The answer style should be like the following examples:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LCEL?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt.invoke(\"What is langgraph?\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ced147",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34196f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the DevOps engineer bring a ladder to work?\n",
      "\n",
      "Because the deployment went to production and they needed to scale!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "response = client_openai.responses.create(\n",
    "    model=LLM_WORKHORSE,\n",
    "    input=\"Tell me a joke about devops\",\n",
    "    # temperature=0.2, # https://community.openai.com/t/temperature-in-gpt-5-models/1337133\n",
    "    max_output_tokens=128,\n",
    "    reasoning={\"effort\": \"minimal\" } # no none\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf93e46",
   "metadata": {},
   "source": [
    "[Reasoning models](https://platform.openai.com/docs/guides/reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc1df126-aaf0-4b2b-90b8-fa3e70bf56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=LLM_WORKHORSE,\n",
    "    # temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    )\n",
    "\n",
    "# How to call the LLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd8a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatMistralAI(model=\"mistral-medium-2508\")\n",
    "\n",
    "# How to call the LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43397ab5",
   "metadata": {},
   "source": [
    "## Calling a Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d9fcd",
   "metadata": {},
   "source": [
    "**invoke (synchronous single input)**\n",
    "\n",
    "* Runs the chain once, blocking until it finishes.\n",
    "* Input = single dict or string (depending on your chain).\n",
    "* Output = single result.\n",
    "\n",
    "✅ Use when you just need one response and don’t care about concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e0095ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'type': 'text', 'text': 'LangChain is an open‑source framework designed to help developers build applications that use large language models (LLMs) more effectively. Instead of treating an LLM as a single “text in → text out” component, LangChain provides modular building blocks and abstractions for composing LLMs into richer, production‑ready applications.\\n\\nKey ideas and components\\n- Chains: Pipelines that connect multiple steps (prompting an LLM, parsing output, calling tools, storing state, etc.) into a single workflow.\\n- Prompts & Prompt Templates: Utilities', 'annotations': []}] additional_kwargs={'reasoning': {'id': 'rs_68bf4a3dab4481969be54474b98a844907d3d114d6c354ae', 'summary': [], 'type': 'reasoning'}} response_metadata={'id': 'resp_68bf4a3c220c8196b97ec395e062816c07d3d114d6c354ae', 'created_at': 1757366844.0, 'incomplete_details': {'reason': 'max_output_tokens'}, 'metadata': {}, 'model': 'gpt-5-mini-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'incomplete', 'model_name': 'gpt-5-mini-2025-08-07'} id='msg_68bf4a3de4e08196a632144ebb3c7b9f07d3d114d6c354ae' usage_metadata={'input_tokens': 11, 'output_tokens': 116, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is LangChain?\", reasoning={\"effort\": \"minimal\" })\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcd302",
   "metadata": {},
   "source": [
    "**ainvoke (asynchronous single input)**\n",
    "\n",
    "* Async version of invoke.\n",
    "* Returns a coroutine → you must await it (inside async def).\n",
    "* Non-blocking → allows parallel I/O (important for web apps, APIs).\n",
    "\n",
    "✅ Use when building async applications (FastAPI, Streamlit, etc.) or when you want multiple requests in parallel.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    result = await llm.ainvoke({\"question\": \"What is LCEL?\"})\n",
    "    print(result)\n",
    "\n",
    "asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cce7bd",
   "metadata": {},
   "source": [
    "**batch (synchronous multiple inputs)**\n",
    "\n",
    "* Run the chain on a list of inputs (e.g., multiple questions).\n",
    "* Executes them one by one under the hood (but can be parallelized with config).\n",
    "* Returns a list of results in the same order.\n",
    "\n",
    "✅ Use when you have a list of tasks and don’t need async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c75bb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'type': 'text', 'text': 'LangChain is an open-source framework designed to help developers build applications that combine large language models (LLMs) with other components and data sources. It provides abstractions and utilities to make it easier to compose chains of operations, connect LLMs to external tools and memory, handle prompts, manage state, and work with documents and retrieval.\\n\\nKey concepts and features:\\n- Chains: Composable workflows where outputs of one step feed into the next (prompt → LLM → post-processing → tool, etc.).\\n- Prompts & Prompt Templates: Utilities to build', 'annotations': []}] additional_kwargs={'reasoning': {'id': 'rs_68bf4a41174c8196bda0c44114d0b21702aace95cabcd7f6', 'summary': [], 'type': 'reasoning'}} response_metadata={'id': 'resp_68bf4a407f1481968685ec2581aca86602aace95cabcd7f6', 'created_at': 1757366848.0, 'incomplete_details': {'reason': 'max_output_tokens'}, 'metadata': {}, 'model': 'gpt-5-mini-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'incomplete', 'model_name': 'gpt-5-mini-2025-08-07'} id='msg_68bf4a4141188196a5d882f1501f434b02aace95cabcd7f6' usage_metadata={'input_tokens': 11, 'output_tokens': 116, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
      "content=[{'type': 'text', 'text': '\"LCEL\" can refer to different things depending on context. Common meanings include:\\n\\n- Low-Complexity Enhancement Layer — in video coding (scalable/video compression) this is an enhancement layer designed to add quality while keeping computational complexity low.\\n- Local Council Electoral List — in some municipal or electoral contexts, LCEL might denote a list used for local council elections.\\n- Localized Clinical Event Log — in healthcare/IT, used to denote logs of clinical events localized to a patient or unit.\\n- Labor Condition Employer List — in HR/immigration', 'annotations': []}] additional_kwargs={'reasoning': {'id': 'rs_68bf4a416230819086cd2ccde00436ea0832bf18b08e8a9c', 'summary': [], 'type': 'reasoning'}} response_metadata={'id': 'resp_68bf4a40872081908e43e0f52dca66c70832bf18b08e8a9c', 'created_at': 1757366848.0, 'incomplete_details': {'reason': 'max_output_tokens'}, 'metadata': {}, 'model': 'gpt-5-mini-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'incomplete', 'model_name': 'gpt-5-mini-2025-08-07'} id='msg_68bf4a4190ec8190b9d1dc350b6314e40832bf18b08e8a9c' usage_metadata={'input_tokens': 11, 'output_tokens': 116, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
      "content=[{'type': 'text', 'text': 'A vector database is a specialized data store designed to store, index, and query high-dimensional vector representations (embeddings) of data instead of—or in addition to—traditional row/column records. These vectors typically come from machine learning models (e.g., word/sentence/image embeddings) and encode semantic information so that items with similar meaning are nearby in vector space.\\n\\nKey features and concepts:\\n- Embeddings: Numeric vectors (commonly 64–4096 dimensions) representing text, images, audio, etc.\\n- Similarity search: Fast nearest-ne', 'annotations': []}] additional_kwargs={'reasoning': {'id': 'rs_68bf4a4124a08193b277336c9591e30a053de15363c5ac22', 'summary': [], 'type': 'reasoning'}} response_metadata={'id': 'resp_68bf4a408cd88193b87f931057063ab9053de15363c5ac22', 'created_at': 1757366848.0, 'incomplete_details': {'reason': 'max_output_tokens'}, 'metadata': {}, 'model': 'gpt-5-mini-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'incomplete', 'model_name': 'gpt-5-mini-2025-08-07'} id='msg_68bf4a41534c8193a3d56ff75d0e6032053de15363c5ac22' usage_metadata={'input_tokens': 12, 'output_tokens': 116, 'total_tokens': 128, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What is LCEL?\",\n",
    "    \"What is a vector database?\"\n",
    "]\n",
    "\n",
    "results = llm.batch(questions,\n",
    "                    config=RunnableConfig(max_concurrency=10),\n",
    "                    reasoning={\"effort\": \"minimal\" }\n",
    "                    )\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d014e73",
   "metadata": {},
   "source": [
    "**There is also:**\n",
    "* abatch → async version of batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501b088",
   "metadata": {},
   "source": [
    "**stream (synchronous streaming)** \n",
    "* Instead of waiting for the entire response, you get tokens/chunks as they arrive.\n",
    "* Great for CLI apps or cases where you want immediate output.\n",
    "\n",
    "\n",
    "```python\n",
    "# Streaming call\n",
    "for chunk in chain.stream({\"question\": \"Explain LangChain Expression Language in simple terms.\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "Here, token by token results are returned as generated, and the application is blocked. It is usefull when developing a cli\n",
    "\n",
    "** astream (asynchronous streaming) **\n",
    "* Same as stream, but async-friendly.\n",
    "* Perfect for web apps (FastAPI, Streamlit, etc.) where you want token-by-token output and not block the application.\n",
    "\n",
    "```python \n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    async for chunk in chain.astream({\"question\": \"Give me a short poem about LCEL.\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n---\\nDone!\")\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7652e9",
   "metadata": {},
   "source": [
    "| Method    | Input       | Output style              | Use case                   |\n",
    "| --------- | ----------- | ------------------------- | -------------------------- |\n",
    "| `invoke`  | 1 input     | 1 final result            | Simple calls               |\n",
    "| `ainvoke` | 1 input     | 1 final result            | Async apps                 |\n",
    "| `batch`   | many inputs | list of results           | Bulk jobs                  |\n",
    "| `abatch`  | many inputs | list of results           | Async bulk                 |\n",
    "| `stream`  | 1 input     | generator of chunks       | CLI / sync streaming       |\n",
    "| `astream` | 1 input     | async generator of chunks | Web apps / async streaming |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5babec",
   "metadata": {},
   "source": [
    "# Chaining:\n",
    "\n",
    "* Chaining means linking multiple components (prompt templates, LLMs, output parsers, retrievers, tools, etc.) together into a pipeline.\n",
    "* The pipe operator (|) is the heart of LCEL — it lets you compose these components like LEGO blocks.\n",
    "* Each component is a Runnable (anything that can accept input and produce output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52c03182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An interesting fact about the Roman Empire is that it had an extensive and sophisticated network of roads—over 250,000 miles at its peak! These roads were so well constructed that some are still in use today. The phrase \"All roads lead to Rome\" comes from this impressive infrastructure, which helped the Romans efficiently manage their vast empire by facilitating trade, military movement, and communication.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 15, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CDdhNKJt3u0aJuiBTZQhVY5kKZtWS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8e6dbcd9-790c-4543-a06f-4f55914c73d3-0', usage_metadata={'input_tokens': 15, 'output_tokens': 77, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini-2025-04-14\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    )\n",
    "\n",
    "\n",
    "chat = prompt | llm \n",
    "\n",
    "chat.invoke(input=\"Roman Empire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f539997d-b85e-480a-88fa-78099fc77232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An interesting fact about the Roman Empire is that it had an extensive and sophisticated network of roads—over 250,000 miles at its peak! These roads were so well constructed that some are still in use today. The phrase \"All roads lead to Rome\" comes from this impressive infrastructure, which helped the Romans efficiently manage their vast empire by facilitating trade, military movement, and communication.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 15, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CDdhxCRuNMj8k4KnNos7gxe6EJbGZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dbe059b9-6d7b-4ae6-9704-906488974bf0-0', usage_metadata={'input_tokens': 15, 'output_tokens': 77, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(input={\"topic\": \"Roman Empire\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fb34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab-rag-e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
