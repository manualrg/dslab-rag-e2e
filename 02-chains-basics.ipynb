{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f10199-9bb5-488b-99ad-f43a16f71205",
   "metadata": {},
   "source": [
    "# LCEL and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8ae065-59d9-473a-a3ad-cef76ff962cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f434c7-b90c-4d59-aa61-a7c19158dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_mistralai import  ChatMistralAI\n",
    "from src import utils, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d368b0",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f55ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_settings = conf.load(file=\"settings.yaml\")\n",
    "conf_settings\n",
    "\n",
    "LLM_WORKHORSE = conf_settings.llm_workhorse\n",
    "LLM_FLAGSHIP = conf_settings.llm_flagship\n",
    "EMBEDDINGS = conf_settings.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18ae9c",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "766b9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce456f-9041-4190-9ff6-6b4df407e747",
   "metadata": {},
   "source": [
    "# What is a Langchain chain\n",
    "\n",
    "It is a composition element that allow to build an structured pipeline to perform IA Generative tasks, specially (but not only) for RAGs\n",
    "\n",
    "\n",
    "Langchain chains are built (in version 1.x or above) using LCEL (LangChain Expression Language)\n",
    "\n",
    "Its core principles are: composability, streaming, async, parallelism\n",
    "\n",
    "The main chains are abstractions layers for:\n",
    "* LLMs\n",
    "* Prompts\n",
    "* VectorStores (Retriever + Embedding)\n",
    "* Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2854e",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4148496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you tell me the distance from the Earth to the Moon?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_chat_hist = [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{question}\")  # variables syntax\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt.invoke(\"Can you tell me the distance from the Earth to the Moon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168e82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "prompt.format_prompt(topic=\"Devops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4278186a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"topic\": \"Devops\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a24de53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input={\"topic\": \"Devops\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca44a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'prompt' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prompt.invoke(topic=\"Devops\")\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950198a",
   "metadata": {},
   "source": [
    "## FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebf267e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define examples\n",
    "examples = [  # input/output keys\n",
    "    {\"input\": \"Q: What is LangChain?\", \"output\": \"A: LangChain is a framework for building applications powered by large language models (LLMs).\"},\n",
    "    {\"input\": \"Q: What is LCEL?\", \"output\": \"A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.\"},\n",
    "]\n",
    "\n",
    "# 2. Create an example prompt template: input/output keys\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "# 3. Few-shot wrapper\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "# 4. Final prompt template (instructions + few-shots + new user question)\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise AI assistant. Answer clearly.\\\n",
    "     The answer style should be like the following examples:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae4a200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt.invoke(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d836514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LCEL?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(few_shot_prompt\n",
    "          .format_prompt() \n",
    "          .to_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae6f6ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a concise AI assistant. Answer clearly.     The answer style should be like the following examples:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LCEL?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt.invoke(\"What is langgraph?\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ced147",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34196f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a DevOps joke for you:\n",
      "\n",
      "Why do DevOps engineers never get lost?\n",
      "\n",
      "Because they always follow the *pipeline*! 😄\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI  # native\n",
    "\n",
    "client_openai = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "response = client_openai.responses.create(\n",
    "    model=LLM_WORKHORSE,\n",
    "    input=\"Tell me a joke about devops\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=128,\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc1df126-aaf0-4b2b-90b8-fa3e70bf56ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why do DevOps engineers never play hide and seek?\\n\\nBecause good luck hiding when everything is monitored and logged!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 14, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CF09uOOlVwYIxOdmGhZtmFnQ3sN8g', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c74f3d30-91c9-4458-96c5-6e1b6b35f2d5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 22, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=LLM_WORKHORSE,\n",
    "    # temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    )\n",
    "\n",
    "# How to call the LLM in langchain?\n",
    "\n",
    "llm.invoke(\"Tell me a joke about devops\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd8a058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a DevOps joke for you:\\n\\n**Why did the DevOps engineer get locked out of the server room?**\\nBecause they forgot their *ssh* keys! 🔑😆\\n\\n*(Bonus groan-worthy follow-up: And then they had to `chmod` their way back in!)*\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 10, 'total_tokens': 76, 'completion_tokens': 66}, 'model_name': 'mistral-medium-2508', 'model': 'mistral-medium-2508', 'finish_reason': 'stop'}, id='run--d46233f8-a7b4-49a1-9b3c-b5ef86c8a475-0', usage_metadata={'input_tokens': 10, 'output_tokens': 66, 'total_tokens': 76})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2 = ChatMistralAI(model=\"mistral-medium-2508\")\n",
    "\n",
    "# How to call the LLM?\n",
    "llm2.invoke(\"Tell me a joke about devops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43397ab5",
   "metadata": {},
   "source": [
    "## Calling a Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d9fcd",
   "metadata": {},
   "source": [
    "**invoke (synchronous single input)**\n",
    "\n",
    "* Runs the chain once, blocking until it finishes.\n",
    "* Input = single dict or string (depending on your chain).\n",
    "* Output = single result.\n",
    "\n",
    "✅ Use when you just need one response and don’t care about concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e0095ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework designed to simplify the development of applications that use large language models (LLMs). It provides tools and abstractions to help developers build applications such as chatbots, summarizers, question-answering systems, and other AI-powered tools that leverage language models effectively.\\n\\nKey features of LangChain include:\\n\\n- **Chain Building:** Enables combining multiple components like prompt templates, LLM calls, and memory into sequences or “chains” to accomplish complex workflows.\\n- **Prompt Management:** Helps structure and reuse prompts efficiently, improving the quality of interactions with language models.\\n- **Memory:** Supports maintaining conversational state or other context across interactions.\\n' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 12, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CExM9lgPDQjjBwOZhL6qneLyxYNyA', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--1e1945ce-13bb-4067-adcc-02cc3d838120-0' usage_metadata={'input_tokens': 12, 'output_tokens': 128, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is LangChain?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcd302",
   "metadata": {},
   "source": [
    "**ainvoke (asynchronous single input)**\n",
    "\n",
    "* Async version of invoke.\n",
    "* Returns a coroutine → you must await it (inside async def).\n",
    "* Non-blocking → allows parallel I/O (important for web apps, APIs).\n",
    "\n",
    "✅ Use when building async applications (FastAPI, Streamlit, etc.) or when you want multiple requests in parallel.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    result = await llm.ainvoke({\"question\": \"What is LCEL?\"})\n",
    "    print(result)\n",
    "\n",
    "asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cce7bd",
   "metadata": {},
   "source": [
    "**batch (synchronous multiple inputs)**\n",
    "\n",
    "* Run the chain on a list of inputs (e.g., multiple questions).\n",
    "* Executes them one by one under the hood (but can be parallelized with config).\n",
    "* Returns a list of results in the same order.\n",
    "\n",
    "✅ Use when you have a list of tasks and don’t need async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c75bb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework designed to simplify the development of applications that use large language models (LLMs). It provides tools and abstractions to help developers build complex, chain-based workflows that combine multiple calls to language models, integrate with external data sources, manage conversational memory, and connect to various APIs or databases.\\n\\nKey features of LangChain include:\\n\\n- **Chains:** Enables combining multiple calls or logical steps involving LLMs into workflows.\\n- **Agents:** Allows dynamic decision-making by LLMs to choose actions or tools based on user input.\\n- **Memory:** Supports maintaining conversational context across interactions.\\n- **Data Integration:** Facilitates connecting' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 12, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CF0Ck5A4Kwnw6vvDjPnVGZSB1fBXX', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--3d1e8c85-d451-4fd2-927e-8de780672403-0' usage_metadata={'input_tokens': 12, 'output_tokens': 128, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='LCEL can refer to different things depending on the context. Could you please specify the field or area you are referring to? For example, it might be an acronym related to education, technology, an organization, or something else. Providing more context will help me give you a precise answer.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 12, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_ed35aa4f5f', 'id': 'chatcmpl-CF0CkJxC4hx5chhjUKogh1ceCAR4c', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--d640b2f5-9eed-474e-a1e1-fa8580888d0a-0' usage_metadata={'input_tokens': 12, 'output_tokens': 58, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='A **vector database** is a specialized type of database designed to store, index, and query high-dimensional vector representations of data. These vectors typically represent complex information such as text, images, audio, or other unstructured data that have been transformed into numerical formats (embeddings) by machine learning models.\\n\\n### Key Characteristics of Vector Databases:\\n- **Storage of Vectors:** They store data points as vectors, often consisting of hundreds or thousands of dimensions.\\n- **Similarity Search:** They enable efficient similarity search, such as *nearest neighbor* or *approximate nearest neighbor* (ANN) searches, which find vectors most similar to a' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 13, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CF0CkS6rayEALoq5bx8tYhUs0RHQL', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--43575180-ec84-47af-a948-266ca408c011-0' usage_metadata={'input_tokens': 13, 'output_tokens': 128, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What is LCEL?\",\n",
    "    \"What is a vector database?\"\n",
    "]\n",
    "\n",
    "results = llm.batch(questions,\n",
    "                    config=RunnableConfig(max_concurrency=10),\n",
    "                    )\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d014e73",
   "metadata": {},
   "source": [
    "**There is also:**\n",
    "* abatch → async version of batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501b088",
   "metadata": {},
   "source": [
    "**stream (synchronous streaming)** \n",
    "* Instead of waiting for the entire response, you get tokens/chunks as they arrive.\n",
    "* Great for CLI apps or cases where you want immediate output.\n",
    "\n",
    "\n",
    "```python\n",
    "# Streaming call\n",
    "for chunk in chain.stream({\"question\": \"Explain LangChain Expression Language in simple terms.\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "Here, token by token results are returned as generated, and the application is blocked. It is usefull when developing a cli\n",
    "\n",
    "** astream (asynchronous streaming) **\n",
    "* Same as stream, but async-friendly.\n",
    "* Perfect for web apps (FastAPI, Streamlit, etc.) where you want token-by-token output and not block the application.\n",
    "\n",
    "```python \n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    async for chunk in chain.astream({\"question\": \"Give me a short poem about LCEL.\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n---\\nDone!\")\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7652e9",
   "metadata": {},
   "source": [
    "| Method    | Input       | Output style              | Use case                   |\n",
    "| --------- | ----------- | ------------------------- | -------------------------- |\n",
    "| `invoke`  | 1 input     | 1 final result            | Simple calls               |\n",
    "| `ainvoke` | 1 input     | 1 final result            | Async apps                 |\n",
    "| `batch`   | many inputs | list of results           | Bulk jobs                  |\n",
    "| `abatch`  | many inputs | list of results           | Async bulk                 |\n",
    "| `stream`  | 1 input     | generator of chunks       | CLI / sync streaming       |\n",
    "| `astream` | 1 input     | async generator of chunks | Web apps / async streaming |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5babec",
   "metadata": {},
   "source": [
    "# Chaining:\n",
    "\n",
    "* Chaining means linking multiple components (prompt templates, LLMs, output parsers, retrievers, tools, etc.) together into a pipeline.\n",
    "* The pipe operator (|) is the heart of LCEL — it lets you compose these components like LEGO blocks.\n",
    "* Each component is a Runnable (anything that can accept input and produce output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52c03182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An interesting fact about the Roman Empire is that it had an extensive and sophisticated network of roads—over 250,000 miles at its peak! These roads were so well constructed that some are still in use today. The phrase \"All roads lead to Rome\" comes from this impressive infrastructure, which helped the Romans efficiently manage their vast empire by facilitating trade, military movement, and communication.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 15, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CExMF6KV8CxR4gytUnPnJiUyWudKv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--79517526-64ec-442d-8b9f-fcbb0543bdf4-0', usage_metadata={'input_tokens': 15, 'output_tokens': 77, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_WORKHORSE,\n",
    "    temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    )\n",
    "\n",
    "\n",
    "chat = prompt | llm \n",
    "\n",
    "chat.invoke(input=\"Roman Empire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f539997d-b85e-480a-88fa-78099fc77232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An interesting fact about the Roman Empire is that it had an extensive and sophisticated road network spanning over 250,000 miles, with about 50,000 miles of paved roads. These roads were so well constructed that some of them are still in use today! The phrase \"All roads lead to Rome\" comes from this impressive infrastructure, which helped the Romans efficiently manage their vast empire by facilitating trade, military movement, and communication.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 15, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CExMHWe8HdlyrpYnTEgJvEVuqqvJq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--290c598f-ac32-4206-a498-f6f6a4052bdf-0', usage_metadata={'input_tokens': 15, 'output_tokens': 86, 'total_tokens': 101, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(input={\"topic\": \"Roman Empire\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9deffefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "912fb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddins!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab-rag-e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
