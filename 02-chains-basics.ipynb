{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f10199-9bb5-488b-99ad-f43a16f71205",
   "metadata": {},
   "source": [
    "# LCEL and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8ae065-59d9-473a-a3ad-cef76ff962cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f434c7-b90c-4d59-aa61-a7c19158dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_mistralai import  ChatMistralAI\n",
    "from src import utils, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d368b0",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f55ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_settings = conf.load(file=\"settings.yaml\")\n",
    "conf_settings\n",
    "\n",
    "LLM_WORKHORSE = conf_settings.llm_workhorse\n",
    "LLM_FLAGSHIP = conf_settings.llm_flagship\n",
    "EMBEDDINGS = conf_settings.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18ae9c",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "766b9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce456f-9041-4190-9ff6-6b4df407e747",
   "metadata": {},
   "source": [
    "# What is a Langchain chain\n",
    "\n",
    "It is a composition element that allow to build an structured pipeline to perform IA Generative tasks, specially (but not only) for RAGs\n",
    "\n",
    "\n",
    "Langchain chains are built (in version 1.x or above) using LCEL (LangChain Expression Language)\n",
    "\n",
    "Its core principles are: composability, streaming, async, parallelism\n",
    "\n",
    "The main chains are abstractions layers for:\n",
    "* LLMs\n",
    "* Prompts\n",
    "* VectorStores (Retriever + Embedding)\n",
    "* Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2854e",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4148496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you tell me the distance from the Earth to the Moon?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_chat_hist = [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{question}\")  # variables syntax\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt.invoke(\"Can you tell me the distance from the Earth to the Moon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168e82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "prompt.format_prompt(topic=\"Devops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4278186a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"topic\": \"Devops\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a24de53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input={\"topic\": \"Devops\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ca44a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasePromptTemplate.invoke() missing 1 required positional argument: 'input'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prompt.invoke(topic=\"Devops\")\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950198a",
   "metadata": {},
   "source": [
    "## FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebf267e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define examples\n",
    "examples = [  # input/output keys\n",
    "    {\"input\": \"Q: What is LangChain?\", \"output\": \"A: LangChain is a framework for building applications powered by large language models (LLMs).\"},\n",
    "    {\"input\": \"Q: What is LCEL?\", \"output\": \"A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.\"},\n",
    "]\n",
    "\n",
    "# 2. Create an example prompt template: input/output keys\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "# 3. Few-shot wrapper\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "# 4. Final prompt template (instructions + few-shots + new user question)\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise AI assistant. Answer clearly.\\\n",
    "     The answer style should be like the following examples:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae4a200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt.invoke(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d836514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LCEL?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(few_shot_prompt\n",
    "          .format_prompt() \n",
    "          .to_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae6f6ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a concise AI assistant. Answer clearly.     The answer style should be like the following examples:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LCEL?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt.invoke(\"What is langgraph?\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ced147",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34196f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a DevOps joke for you:\n",
      "\n",
      "Why do DevOps engineers never get lost?\n",
      "\n",
      "Because they always follow the *pipeline*! üòÑ\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI  # native\n",
    "\n",
    "client_openai = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "response = client_openai.responses.create(\n",
    "    model=LLM_WORKHORSE,\n",
    "    input=\"Tell me a joke about devops\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=128,\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc1df126-aaf0-4b2b-90b8-fa3e70bf56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=LLM_WORKHORSE,\n",
    "    # temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    )\n",
    "\n",
    "# How to call the LLM in langchain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dd8a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatMistralAI(model=\"mistral-medium-2508\")\n",
    "\n",
    "# How to call the LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43397ab5",
   "metadata": {},
   "source": [
    "## Calling a Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d9fcd",
   "metadata": {},
   "source": [
    "**invoke (synchronous single input)**\n",
    "\n",
    "* Runs the chain once, blocking until it finishes.\n",
    "* Input = single dict or string (depending on your chain).\n",
    "* Output = single result.\n",
    "\n",
    "‚úÖ Use when you just need one response and don‚Äôt care about concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e0095ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework designed to simplify the development of applications that use large language models (LLMs). It provides tools and abstractions to help developers build applications such as chatbots, summarizers, question-answering systems, and other AI-powered tools that leverage language models effectively.\\n\\nKey features of LangChain include:\\n\\n- **Chain Building:** Enables combining multiple components like prompt templates, LLM calls, and memory into sequences or ‚Äúchains‚Äù to accomplish complex workflows.\\n- **Prompt Management:** Helps structure and reuse prompts efficiently, improving the quality of interactions with language models.\\n- **Memory:** Supports maintaining conversational state or other context across interactions.\\n' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 12, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CExM9lgPDQjjBwOZhL6qneLyxYNyA', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--1e1945ce-13bb-4067-adcc-02cc3d838120-0' usage_metadata={'input_tokens': 12, 'output_tokens': 128, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is LangChain?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcd302",
   "metadata": {},
   "source": [
    "**ainvoke (asynchronous single input)**\n",
    "\n",
    "* Async version of invoke.\n",
    "* Returns a coroutine ‚Üí you must await it (inside async def).\n",
    "* Non-blocking ‚Üí allows parallel I/O (important for web apps, APIs).\n",
    "\n",
    "‚úÖ Use when building async applications (FastAPI, Streamlit, etc.) or when you want multiple requests in parallel.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    result = await llm.ainvoke({\"question\": \"What is LCEL?\"})\n",
    "    print(result)\n",
    "\n",
    "asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cce7bd",
   "metadata": {},
   "source": [
    "**batch (synchronous multiple inputs)**\n",
    "\n",
    "* Run the chain on a list of inputs (e.g., multiple questions).\n",
    "* Executes them one by one under the hood (but can be parallelized with config).\n",
    "* Returns a list of results in the same order.\n",
    "\n",
    "‚úÖ Use when you have a list of tasks and don‚Äôt need async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c75bb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is an open-source framework designed to simplify the development of applications that use large language models (LLMs). It provides tools and abstractions to help developers build complex language model-powered applications by connecting LLMs with other components like data, APIs, and user interfaces.\\n\\nKey features of LangChain include:\\n\\n1. **Modularity:** LangChain offers building blocks such as prompt templates, chains (sequences of calls to language models or other utilities), agents (which can decide which actions to take), and memory components (to maintain state over interactions).\\n\\n2. **Integration:** It facilitates easy integration of LLMs with external data sources' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 12, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CExMCnoznv6DskhWsSfvfSO0sfWdO', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--89d277d8-b9e5-4cc5-8709-cd8d17c5ca66-0' usage_metadata={'input_tokens': 12, 'output_tokens': 128, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='LCEL can stand for different things depending on the context. Could you please specify the field or area you are referring to? For example, LCEL might refer to:\\n\\n- **Low-Cost Embedded Linux**: A minimalist Linux distribution for embedded systems.\\n- **Laboratory for Computational and Experimental Linguistics**: A research lab or group at certain universities.\\n- **Living Cost and Environmental Lab**: Hypothetical or specific project names related to environmental studies.\\n- **Any organization or program acronym** specific to certain industries or regions.\\n\\nIf you provide more details, I can give a more precise explanation.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 12, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_a150906e27', 'id': 'chatcmpl-CExMCmOtCJX52U47qQWdWWfDcz9Ga', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--62eaa21c-91b0-4a49-b993-48d24538c703-0' usage_metadata={'input_tokens': 12, 'output_tokens': 122, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='A **vector database** is a specialized type of database designed to store, index, and query data represented as high-dimensional vectors. These vectors typically encode complex information such as text, images, audio, or other unstructured data into numerical forms that capture semantic meaning, relationships, or features.\\n\\n### Why vectors?\\nIn many modern applications, especially in machine learning and AI, data is often transformed into vector representations (also called embeddings). For example:\\n- A sentence can be converted into a vector that captures its meaning (using models like BERT, GPT, or word2vec).\\n- An image can be encoded as a feature vector by a' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 13, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CExMChyoJ82wFPWcSqr3PVN1yAmcL', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--69824778-b323-46be-a004-1fc45a8cd192-0' usage_metadata={'input_tokens': 13, 'output_tokens': 128, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What is LCEL?\",\n",
    "    \"What is a vector database?\"\n",
    "]\n",
    "\n",
    "results = llm.batch(questions,\n",
    "                    config=RunnableConfig(max_concurrency=10),\n",
    "                    )\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d014e73",
   "metadata": {},
   "source": [
    "**There is also:**\n",
    "* abatch ‚Üí async version of batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501b088",
   "metadata": {},
   "source": [
    "**stream (synchronous streaming)** \n",
    "* Instead of waiting for the entire response, you get tokens/chunks as they arrive.\n",
    "* Great for CLI apps or cases where you want immediate output.\n",
    "\n",
    "\n",
    "```python\n",
    "# Streaming call\n",
    "for chunk in chain.stream({\"question\": \"Explain LangChain Expression Language in simple terms.\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "Here, token by token results are returned as generated, and the application is blocked. It is usefull when developing a cli\n",
    "\n",
    "** astream (asynchronous streaming) **\n",
    "* Same as stream, but async-friendly.\n",
    "* Perfect for web apps (FastAPI, Streamlit, etc.) where you want token-by-token output and not block the application.\n",
    "\n",
    "```python \n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    async for chunk in chain.astream({\"question\": \"Give me a short poem about LCEL.\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n---\\nDone!\")\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7652e9",
   "metadata": {},
   "source": [
    "| Method    | Input       | Output style              | Use case                   |\n",
    "| --------- | ----------- | ------------------------- | -------------------------- |\n",
    "| `invoke`  | 1 input     | 1 final result            | Simple calls               |\n",
    "| `ainvoke` | 1 input     | 1 final result            | Async apps                 |\n",
    "| `batch`   | many inputs | list of results           | Bulk jobs                  |\n",
    "| `abatch`  | many inputs | list of results           | Async bulk                 |\n",
    "| `stream`  | 1 input     | generator of chunks       | CLI / sync streaming       |\n",
    "| `astream` | 1 input     | async generator of chunks | Web apps / async streaming |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5babec",
   "metadata": {},
   "source": [
    "# Chaining:\n",
    "\n",
    "* Chaining means linking multiple components (prompt templates, LLMs, output parsers, retrievers, tools, etc.) together into a pipeline.\n",
    "* The pipe operator (|) is the heart of LCEL ‚Äî it lets you compose these components like LEGO blocks.\n",
    "* Each component is a Runnable (anything that can accept input and produce output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52c03182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An interesting fact about the Roman Empire is that it had an extensive and sophisticated network of roads‚Äîover 250,000 miles at its peak! These roads were so well constructed that some are still in use today. The phrase \"All roads lead to Rome\" comes from this impressive infrastructure, which helped the Romans efficiently manage their vast empire by facilitating trade, military movement, and communication.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 15, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CExMF6KV8CxR4gytUnPnJiUyWudKv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--79517526-64ec-442d-8b9f-fcbb0543bdf4-0', usage_metadata={'input_tokens': 15, 'output_tokens': 77, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_WORKHORSE,\n",
    "    temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    )\n",
    "\n",
    "\n",
    "chat = prompt | llm \n",
    "\n",
    "chat.invoke(input=\"Roman Empire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f539997d-b85e-480a-88fa-78099fc77232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An interesting fact about the Roman Empire is that it had an extensive and sophisticated road network spanning over 250,000 miles, with about 50,000 miles of paved roads. These roads were so well constructed that some of them are still in use today! The phrase \"All roads lead to Rome\" comes from this impressive infrastructure, which helped the Romans efficiently manage their vast empire by facilitating trade, military movement, and communication.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 15, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CExMHWe8HdlyrpYnTEgJvEVuqqvJq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--290c598f-ac32-4206-a498-f6f6a4052bdf-0', usage_metadata={'input_tokens': 15, 'output_tokens': 86, 'total_tokens': 101, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(input={\"topic\": \"Roman Empire\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "912fb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddins!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab-rag-e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
