{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f10199-9bb5-488b-99ad-f43a16f71205",
   "metadata": {},
   "source": [
    "# LCEL and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8ae065-59d9-473a-a3ad-cef76ff962cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f434c7-b90c-4d59-aa61-a7c19158dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_mistralai import  ChatMistralAI\n",
    "from src import utils, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d368b0",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f55ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_settings = conf.load(file=\"settings.yaml\")\n",
    "conf_settings\n",
    "\n",
    "LLM_WORKHORSE = conf_settings.llm_workhorse\n",
    "LLM_FLAGSHIP = conf_settings.llm_flagship\n",
    "EMBEDDINGS = conf_settings.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18ae9c",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "766b9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce456f-9041-4190-9ff6-6b4df407e747",
   "metadata": {},
   "source": [
    "# What is a Langchain chain\n",
    "\n",
    "It is a composition element that allow to build an structured pipeline to perform IA Generative tasks, specially (but not only) for RAGs\n",
    "\n",
    "\n",
    "LAngchain chains are built (in version 1.x or above) using LCEL (LangChain Expression Language)\n",
    "\n",
    "Its core principles are: composability, streaming, async, parallelism\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2854e",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4148496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you tell me the distance from the Earth to the Moon?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_chat_hist = [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"user\", \"{question}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt.invoke(\"Can you tell me the distance from the Earth to the Moon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168e82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "prompt.format_prompt(topic=\"Devops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4278186a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me an interesting fact about Devops', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"topic\": \"Devops\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950198a",
   "metadata": {},
   "source": [
    "## FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf267e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define examples\n",
    "examples = [  # input/output keys\n",
    "    {\"input\": \"Q: What is LangChain?\", \"output\": \"A: LangChain is a framework for building applications powered by large language models (LLMs).\"},\n",
    "    {\"input\": \"Q: What is LCEL?\", \"output\": \"A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.\"},\n",
    "]\n",
    "\n",
    "# 2. Create an example prompt template\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "# 3. Few-shot wrapper\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "# 4. Final prompt template (instructions + few-shots + new user question)\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise AI assistant. Answer clearly.\\\n",
    "     The answer style should be like the following examples:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae4a200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt.invoke(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d836514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LCEL?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(few_shot_prompt\n",
    "          .format_prompt() \n",
    "          .to_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae6f6ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a concise AI assistant. Answer clearly.     The answer style should be like the following examples:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LangChain is a framework for building applications powered by large language models (LLMs).', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Q: What is LCEL?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A: LCEL (LangChain Expression Language) is a way to build chains using composable operators like | for clarity and power.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt.invoke(\"What is langgraph?\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ced147",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34196f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a DevOps joke for you:\n",
      "\n",
      "Why do DevOps engineers never get lost?\n",
      "\n",
      "Because they always follow the *pipeline*! ðŸ˜„\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "response = client_openai.responses.create(\n",
    "    model=LLM_WORKHORSE,\n",
    "    input=\"Tell me a joke about devops\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=128,\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc1df126-aaf0-4b2b-90b8-fa3e70bf56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=LLM_WORKHORSE,\n",
    "    # temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    )\n",
    "\n",
    "# How to call the LLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd8a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatMistralAI(model=\"mistral-medium-2508\")\n",
    "\n",
    "# How to call the LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43397ab5",
   "metadata": {},
   "source": [
    "## Calling a Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d9fcd",
   "metadata": {},
   "source": [
    "**invoke (synchronous single input)**\n",
    "\n",
    "* Runs the chain once, blocking until it finishes.\n",
    "* Input = single dict or string (depending on your chain).\n",
    "* Output = single result.\n",
    "\n",
    "âœ… Use when you just need one response and donâ€™t care about concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e0095ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework designed to simplify the development of applications that use large language models (LLMs). It provides tools and abstractions for building language model-powered apps that can integrate with various data sources, manage conversations, handle memory, execute chains of prompts, and connect to external APIs and knowledge bases.\\n\\nKey features of LangChain include:\\n\\n- **Prompt Management:** Helps structure and manage prompts for better and more consistent outputs from LLMs.\\n- **Chains:** Allows developers to create sequences of calls to language models or other components, enabling complex workflows.\\n- **Memory:** Maintains context across interactions, useful for chatbots and conversational agents' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 12, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CEiutptAPb9sAStdkYvtOZ4DWGYdr', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--e6dd3e4b-6c10-48ea-8d28-6db41cfe8c48-0' usage_metadata={'input_tokens': 12, 'output_tokens': 128, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is LangChain?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcd302",
   "metadata": {},
   "source": [
    "**ainvoke (asynchronous single input)**\n",
    "\n",
    "* Async version of invoke.\n",
    "* Returns a coroutine â†’ you must await it (inside async def).\n",
    "* Non-blocking â†’ allows parallel I/O (important for web apps, APIs).\n",
    "\n",
    "âœ… Use when building async applications (FastAPI, Streamlit, etc.) or when you want multiple requests in parallel.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    result = await llm.ainvoke({\"question\": \"What is LCEL?\"})\n",
    "    print(result)\n",
    "\n",
    "asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cce7bd",
   "metadata": {},
   "source": [
    "**batch (synchronous multiple inputs)**\n",
    "\n",
    "* Run the chain on a list of inputs (e.g., multiple questions).\n",
    "* Executes them one by one under the hood (but can be parallelized with config).\n",
    "* Returns a list of results in the same order.\n",
    "\n",
    "âœ… Use when you have a list of tasks and donâ€™t need async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c75bb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework designed to simplify the development of applications that use large language models (LLMs). It provides tools and abstractions to help developers build complex workflows by integrating LLMs with other components such as data sources, APIs, and external tools.\\n\\nKey features of LangChain include:\\n\\n- ** chaining components:** Easily link together calls to LLMs and other utilities in a defined sequence, enabling complex multi-step processes.\\n- **Prompt management:** Tools to create, manage, and optimize prompts for better LLM performance.\\n- **Memory:** Maintain state or context across interactions, useful for conversational agents.\\n- **Integration with external data' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 12, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_ed35aa4f5f', 'id': 'chatcmpl-CEivMSkwqGVCY3DVqdzGfh8stpvwv', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--67c7e996-4b11-4aab-bb89-7e98d6599d3a-0' usage_metadata={'input_tokens': 12, 'output_tokens': 128, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='LCEL can stand for different things depending on the context, but some common meanings include:\\n\\n1. **London Centre for English Language (LCEL)** â€“ an English language school or institution providing courses for English learners.\\n2. **Lamar County Electric Cooperative (LCEL)** â€“ a utility company providing electric services in certain regions.\\n3. **Low-Cost Experimental Launcher (LCEL)** â€“ a term used in aerospace contexts referring to a budget-friendly rocket or launch system.\\n\\nIf you can provide the context in which you encountered \"LCEL,\" I can give a more precise explanation!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 12, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_ed35aa4f5f', 'id': 'chatcmpl-CEivMrU7fyqBOawUKPNC3DjBE8pXV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--12893c9a-24c2-4912-87e0-7a41e596041a-0' usage_metadata={'input_tokens': 12, 'output_tokens': 117, 'total_tokens': 129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='A **vector database** is a type of database specifically designed to store, index, and query data represented as high-dimensional vectors. These vectors are numerical representations (usually arrays of floating-point numbers) that encode information such as text, images, audio, or other complex data in a way that captures their semantic meaning or features. \\n\\n### Why Vectors?\\nIn many machine learning and AI applications, raw data (like words, sentences, images) is transformed into vectors using techniques like word embeddings (e.g., Word2Vec, GloVe), sentence embeddings (e.g., Sentence-BERT), or image embeddings (e.g., from' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 13, 'total_tokens': 141, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_a150906e27', 'id': 'chatcmpl-CEivMOeHWfvQhRhVNfZISkcxSUnMN', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--03c35fe7-dc7b-4b8c-9c9b-bc09702513a1-0' usage_metadata={'input_tokens': 13, 'output_tokens': 128, 'total_tokens': 141, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What is LCEL?\",\n",
    "    \"What is a vector database?\"\n",
    "]\n",
    "\n",
    "results = llm.batch(questions,\n",
    "                    config=RunnableConfig(max_concurrency=10),\n",
    "                    )\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d014e73",
   "metadata": {},
   "source": [
    "**There is also:**\n",
    "* abatch â†’ async version of batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501b088",
   "metadata": {},
   "source": [
    "**stream (synchronous streaming)** \n",
    "* Instead of waiting for the entire response, you get tokens/chunks as they arrive.\n",
    "* Great for CLI apps or cases where you want immediate output.\n",
    "\n",
    "\n",
    "```python\n",
    "# Streaming call\n",
    "for chunk in chain.stream({\"question\": \"Explain LangChain Expression Language in simple terms.\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "Here, token by token results are returned as generated, and the application is blocked. It is usefull when developing a cli\n",
    "\n",
    "** astream (asynchronous streaming) **\n",
    "* Same as stream, but async-friendly.\n",
    "* Perfect for web apps (FastAPI, Streamlit, etc.) where you want token-by-token output and not block the application.\n",
    "\n",
    "```python \n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    async for chunk in chain.astream({\"question\": \"Give me a short poem about LCEL.\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n---\\nDone!\")\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7652e9",
   "metadata": {},
   "source": [
    "| Method    | Input       | Output style              | Use case                   |\n",
    "| --------- | ----------- | ------------------------- | -------------------------- |\n",
    "| `invoke`  | 1 input     | 1 final result            | Simple calls               |\n",
    "| `ainvoke` | 1 input     | 1 final result            | Async apps                 |\n",
    "| `batch`   | many inputs | list of results           | Bulk jobs                  |\n",
    "| `abatch`  | many inputs | list of results           | Async bulk                 |\n",
    "| `stream`  | 1 input     | generator of chunks       | CLI / sync streaming       |\n",
    "| `astream` | 1 input     | async generator of chunks | Web apps / async streaming |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5babec",
   "metadata": {},
   "source": [
    "# Chaining:\n",
    "\n",
    "* Chaining means linking multiple components (prompt templates, LLMs, output parsers, retrievers, tools, etc.) together into a pipeline.\n",
    "* The pipe operator (|) is the heart of LCEL â€” it lets you compose these components like LEGO blocks.\n",
    "* Each component is a Runnable (anything that can accept input and produce output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52c03182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An interesting fact about the Roman Empire is that it had an extensive and sophisticated network of roadsâ€”over 250,000 miles at its peak! These roads were so well constructed that some of them are still in use today. The phrase \"All roads lead to Rome\" comes from this impressive infrastructure, which helped the Romans efficiently manage their vast empire by facilitating trade, military movement, and communication.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 15, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_a150906e27', 'id': 'chatcmpl-CEivPuKevKuh0oEZw5Pk5ZQQqYH1T', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--31c8265b-3c76-48ba-8615-4449c0bc86bb-0', usage_metadata={'input_tokens': 15, 'output_tokens': 79, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_WORKHORSE,\n",
    "    temperature=0.2,\n",
    "    max_tokens=128,\n",
    "    )\n",
    "\n",
    "\n",
    "chat = prompt | llm \n",
    "\n",
    "chat.invoke(input=\"Roman Empire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f539997d-b85e-480a-88fa-78099fc77232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An interesting fact about the Roman Empire is that they built an extensive network of roadsâ€”over 250,000 miles at its peak! These roads were so well constructed that many of them are still in use today, forming the basis for modern European road systems. The famous phrase \"All roads lead to Rome\" reflects how central and connected the city was within this vast network.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 15, 'total_tokens': 90, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_a150906e27', 'id': 'chatcmpl-CEivR6zrUpPFyjVY8As6LYPvhKxR8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--aa641c7b-33d9-4171-9ea8-267fb2889951-0', usage_metadata={'input_tokens': 15, 'output_tokens': 75, 'total_tokens': 90, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(input={\"topic\": \"Roman Empire\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fb34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab-rag-e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
