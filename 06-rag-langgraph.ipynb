{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df6dc35",
   "metadata": {},
   "source": [
    "# Implement a basic RAG by using LG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048ab727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba54eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqdrant_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QdrantClient\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqdrant_client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhttp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Distance, VectorParams\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, conf, sample_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manuelalberto.romero\\Documents\\repos\\dslabs\\dslab-rag-e2e\\src\\sample_docs.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m      4\u001b[39m uuids = [\u001b[33m'\u001b[39m\u001b[33m2690cf82-ebfd-48bc-bd52-c61a595a212a\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m  \u001b[33m'\u001b[39m\u001b[33m0e8f454e-3ebf-434b-a7cf-26489695bcd0\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m docs = [\n\u001b[32m      7\u001b[39m     Document(\n\u001b[32m      8\u001b[39m         page_content=\u001b[33m\"\u001b[39m\u001b[33mJohn J. Hopfield and Geoffrey Hinton received the Nobel Prize in Physics in 2024 for their groundbreaking work on artificial neural networks, a foundation of modern AI. Hopfield developed an associative memory model in the 1980s that allows networks to store and reconstruct patterns. Building on this, Hinton developed the Boltzmann machine, which uses statistical physics principles to recognize and classify data. These pioneering contributions are essential for today\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms machine learning technologies, enhancing applications from medical imaging to material science.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     ),\n\u001b[32m     15\u001b[39m ]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.schema'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "\n",
    "\n",
    "from src import utils, conf, sample_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10d594",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_settings = conf.load(file=\"settings.yaml\")\n",
    "\n",
    "LLM_WORKHORSE = conf_settings.llm_workhorse\n",
    "LLM_FLAGSHIP = conf_settings.llm_flagship\n",
    "EMBEDDINGS = conf_settings.embeddings\n",
    "EMB_DIM = conf_settings.embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f954a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_infra = conf.load(file=\"infra.yaml\")\n",
    "\n",
    "VDB_URL = conf_infra.vdb_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cf4c6",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76475a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "QDRANT_API_KEY = os.environ[\"QDRANT_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f345e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=EMBEDDINGS\n",
    "    )\n",
    "\n",
    "try:\n",
    "    embeddings.embed_query(\"abc\")\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=LLM_WORKHORSE,\n",
    "    )\n",
    "\n",
    "try:\n",
    "    llm.invoke(\"tell me a joke about devops\")\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_qdrant = QdrantClient(\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    url=VDB_URL\n",
    "    )\n",
    "\n",
    "try:\n",
    "    client_qdrant.get_collections()\n",
    "except Exception as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a57e3",
   "metadata": {},
   "source": [
    "## Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_qdrant.get_collection(\"tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a86bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if client_qdrant.collection_exists(\"tutorial\"):\n",
    "    vector_store = QdrantVectorStore.from_existing_collection(\n",
    "        embedding=embeddings,\n",
    "        collection_name=\"tutorial\",\n",
    "        url=VDB_URL,\n",
    "        api_key=QDRANT_API_KEY\n",
    "    )\n",
    "else:\n",
    "    print(\"run 04-rag-langchain!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = vector_store.similarity_search(\"who won the 2024 Nobel in Chemistry?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908576b",
   "metadata": {},
   "source": [
    "# Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fe747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67227c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    # access only needed info\n",
    "    context = state[\"context\"]  # List(Documents)\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    docs_content = format_docs(context)\n",
    "    messages = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"question\": \"who won the 2024 Nobel in Chemistry?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a187684",
   "metadata": {},
   "source": [
    "# Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafedd6",
   "metadata": {},
   "source": [
    "## Dynamic Source\n",
    "\n",
    "[Structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Filter(BaseModel):\n",
    "    do_filter: bool = Field(\"Whether the user has specified any given source or not\")\n",
    "    value: Optional[str] = Field(\n",
    "        \"Infer from the query is the user intent is to get information only from a given source. Answer only with any of these values: `wikipedia`, `national_geographic`, `desperta_ferro`\")\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Filter)\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template_filt = \"\"\"Your task is to analyze a question and determine whether the user want to retrieve the answer from a given source.\\\n",
    "Do NOT answer the question itself, only the source if it is stated.\\\n",
    "You can provide an empty string if no source is requested by the user.\\\n",
    "Wrap the output in the following json format:\\n {format_instructions}\\\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt_filt = ChatPromptTemplate.from_template(prompt_template_filt)\n",
    "chain_filt = (prompt_filt.partial(format_instructions=parser.get_format_instructions()) \n",
    "         | llm \n",
    "         | parser\n",
    "         )\n",
    "\n",
    "resp = chain_filt.invoke(\"Según Desperta Ferro, existe el término reconquista?\")\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_filt.partial(format_instructions=parser.get_format_instructions()).invoke(\"Según Desperta Ferro, existe el término reconquista?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b68015",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83508a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_filter(state: State):\n",
    "    # removes context not matchin the source in doc.metadata['source']\n",
    "    q = state['question']\n",
    "    print(q)\n",
    "    resp = chain_filt.invoke(q)\n",
    "\n",
    "    if resp.do_filter:\n",
    "        value = resp.value\n",
    "        print(f\"Filter: {value=}\")\n",
    "        print( [doc.metadata for doc in state['context'] ])\n",
    "        docs_filt = [doc for doc in state['context'] if doc.metadata['source'] == value]\n",
    "        print(f\"Remaning docs: {len(docs_filt)=}\")\n",
    "    else:\n",
    "        docs_filt = state['context']\n",
    "\n",
    "    return {\"context\": docs_filt}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, source_filter, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "graph\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"question\": \"According to wikipedia, who won the Chemistry Nobel Prize in 2024?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02669244",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "* Enhance retrieval with native client and custom options (metadata filter)\n",
    "* Add post processing link the answer to images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d472ab7",
   "metadata": {},
   "source": [
    "## Self-reflective RAG\n",
    "\n",
    "We are going to implement a linear version of [Self-Reflective RAG with LangGraph](https://blog.langchain.com/agentic-rag-with-langgraph/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def critic_context_relevance(state: State):\n",
    "    contexts = state[\"context\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    for context in contexts:\n",
    "        # Use a critic LLM to sinthesize a relevancy score for each context individually\n",
    "        # Use prompting to only yield a value between 0 and 3 and in int format\n",
    "        context.metadata['relevance_score'] = 3\n",
    "\n",
    "    return {\"context\": contexts}\n",
    "\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, critic_context_relevance, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "graph.invoke({\"question\": \"According to wikipedia, who won the Chemistry Nobel Prize in 2024?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490253f",
   "metadata": {},
   "source": [
    "## Other ideas:\n",
    "\n",
    "* Two stage retrieval (Documents +  Chunks)\n",
    "* Conversational Flow\n",
    "* Tool calling (Agents!)\n",
    "* Multi-retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d66c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
