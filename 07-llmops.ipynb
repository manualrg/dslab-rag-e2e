{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a80179e",
   "metadata": {},
   "source": [
    "# LLMOps: Observability\n",
    "\n",
    "LLMOps involves a comprehensive set of activities, including:\n",
    "* Model deployment and maintenance: deploying and managing LLMs on cloud platforms or on-premises infrastructure\n",
    "* Data management: curating and preparing training data, as well as monitoring and maintaining data quality\n",
    "* Model training and fine-tuning: training and refining LLMs to improve their performance on specific tasks\n",
    "* Monitoring and evaluation: tracking LLM performance, identifying errors, and optimizing models\n",
    "* Security and compliance: ensuring the security and regulatory compliance of LLM operations\n",
    "[What is LLMOps (large language model operations)?](https://cloud.google.com/discover/what-is-llmops?hl=en)\n",
    "\n",
    "\n",
    "Observability is the broader concept of understanding what is happening under the hood of your LLM application. Traces are the object used to achieve deep observability.\n",
    "A trace is a piece of logged data from a AIGen Workflow (Agent, RAG, so on) to allow development, debugging, monitoring and explainability\n",
    "* Developing and debugging: Complex agentic workflows requiere an easy way to check input/outputs\n",
    "* Monitoring: Ability to drill down in terms on latency and tokens\n",
    "* Explainability: Track back LLMs outputs in a cohesive lineage \n",
    "\n",
    "In addition, using a platform will ease this process and allow a desired level of **reproducibility to experiments**.\n",
    "\n",
    "Most popular platformns:\n",
    "\n",
    "![llmops-platforms](docs/llmops-platforms.png)\n",
    "\n",
    "\n",
    "**STOP PRINTING IN NOTEBOOKS**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7462b",
   "metadata": {},
   "source": [
    "# Langfuse\n",
    "\n",
    "Open Source LLM Engineering Platform: Traces, evals, prompt management and metrics to debug and improve your LLM application\n",
    "![langufse-landscape](docs/langufse-landscape.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048ab727",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba54eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuelalberto.romero\\Documents\\repos\\dslabs\\dslab-rag-e2e\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from src import conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10d594",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8eb5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_infra = conf.load(file=\"infra.yaml\")\n",
    "conf_settings = conf.load(file=\"settings.yaml\")\n",
    "\n",
    "LLM_WORKHORSE = conf_settings.llm_workhorse\n",
    "EMBEDDINGS = conf_settings.embeddings\n",
    "INDEX_NAME = conf_settings.vdb_index\n",
    "LANGFUSE_HOST = conf_infra.llmops_url\n",
    "VDB_URL = conf_infra.vdb_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cf4c6",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76475a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "# LANGFUSE_SECRET_KEY = \n",
    "# LANGFUSE_PUBLIC_KEY = \n",
    "os.environ['LANGFUSE_HOST'] = LANGFUSE_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077a74e",
   "metadata": {},
   "source": [
    "# Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64b09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=LLM_WORKHORSE,\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=EMBEDDINGS)\n",
    "vector_store = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=embeddings,\n",
    "    collection_name=INDEX_NAME,\n",
    "    url=VDB_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e98906c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cloud.langfuse.com'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['LANGFUSE_HOST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003981de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f78a5",
   "metadata": {},
   "source": [
    "# Tracing\n",
    "\n",
    "A **trace** typically represents a single request or operation. It contains the overall input and output of the function, as well as metadata about the request ( i.e. user, session, tags, etc.).  \n",
    "Each trace can contain **multiple observations** to log the individual steps of the execution. Usually, a trace corresponds to a single api call of an application.  \n",
    "**Sessions** are used to group traces that are part of the same user interaction. A common example is a thread in a chat interface.  \n",
    "\n",
    "## Spans: \n",
    "Picece of data that is part of a trace (unit of work). It can be created in different ways:\n",
    "* Manually\n",
    "* Decorator @observe\n",
    "* Context manager\n",
    "* Integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually\n",
    " \n",
    "# Create a span without a context manager\n",
    "span = langfuse.start_span(name=\"user-request\")\n",
    " \n",
    "# Your processing logic here\n",
    "span.update(output=\"Request processed\")\n",
    " \n",
    "# Child spans must be created using the parent span object\n",
    "nested_span = span.start_span(name=\"nested-span\")\n",
    "nested_span.update(output=\"Nested span output\")\n",
    " \n",
    "# Important: Manually end the span\n",
    "nested_span.end()\n",
    " \n",
    "# Important: Manually end the parent span\n",
    "span.end()\n",
    " \n",
    "# Flush events in short-lived applications\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0388d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decorator\n",
    "# a design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure.\n",
    "from langfuse import observe\n",
    "\n",
    "@observe\n",
    "def my_function():\n",
    "    return \"Hello, world!\" # Input/output and timings are automatically captured\n",
    " \n",
    "my_function()\n",
    " \n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba30b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Integrations: Langchain\n",
    "\n",
    "from langfuse.langchain import CallbackHandler\n",
    " \n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    " \n",
    "llm = ChatOpenAI(model_name=LLM_WORKHORSE)\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm\n",
    " \n",
    "response = chain.invoke(\n",
    "    {\"topic\": \"cats\"}, \n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115f6c6",
   "metadata": {},
   "source": [
    "## Sessions, users, tags and other metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f348c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why do DevOps engineers never play hide and seek?\\n\\nBecause good luck hiding when everything is constantly being monitored and deployed!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 14, 'total_tokens': 38, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CEifom0j2ph8okGJBGLTjiiyRCqlB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--23dfbfd0-9057-4723-a59e-f0c20abbf4f9-0', usage_metadata={'input_tokens': 14, 'output_tokens': 24, 'total_tokens': 38, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler = CallbackHandler()\n",
    " \n",
    "ID_SESSION = \"2025-07-30\"\n",
    "ID_USER = \"manualrg\"\n",
    "# Pass langfuse_session_id as metadata to the chain invocation\n",
    "chain.invoke(\n",
    "    {\"topic\": \"devops\"},\n",
    "    config={\n",
    "        \"callbacks\": [handler],\n",
    "        \"metadata\": {\n",
    "            \"langfuse_session_id\": ID_SESSION,\n",
    "            \"langfuse_user_id\": ID_USER,\n",
    "            \"langfuse_tags\": [\"mbit\", \"retrieve-k=3\"],\n",
    "            # keys not matching will be stored at metadata\n",
    "            \"foo\": \"bar\"\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cf08e",
   "metadata": {},
   "source": [
    "# Integration in Langraph\n",
    "\n",
    "It is posible to integrate it like in LC at invokation time, but also, at compile time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3dae6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5124ceb430e2398b70e8d66e95cf3488'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rag import main\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "rag_graph = main(\n",
    "    \"space\", 3\n",
    ")\n",
    "\n",
    "rag_graph.invoke({\"question\": \"is Althera A larger than the Sun?\"},\n",
    "                 config={\n",
    "        \"callbacks\": [handler],\n",
    "        \"metadata\": {\n",
    "            \"langfuse_session_id\": ID_SESSION,\n",
    "            \"langfuse_user_id\": ID_USER,\n",
    "            \"langfuse_tags\": [\"mbit\", \"retrieve-k=3\"],\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "handler.last_trace_id  # check in UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82fa03",
   "metadata": {},
   "source": [
    "#  Scores\n",
    "\n",
    "There are several ways of setting scores, mainly:\n",
    "* online: Can access to the current trace\n",
    "* offline: Have to retrieve a trace id to attach a score\n",
    "\n",
    "The offline version most common in chatbots, because a user can yield a feedback at any moment.\n",
    "We will have to follow the next diagram:\n",
    "\n",
    "```\n",
    "     frontend                   backend                   langfuse\n",
    "        |--------------hu----------->|                        |\n",
    "        |                            |                        |\n",
    "        |                            |--------trace[i]------->|\n",
    "        |                            |<----trace[i].id--------|\n",
    "        |<-----ai, trace_id----------|                        |\n",
    "        |-----feedback, trace_id---->|                        |\n",
    "        |                            |----score, trace_id---->|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb416396",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_score(\n",
    "    trace_id=langfuse_handler.last_trace_id,\n",
    "    name=\"user-feedback\",\n",
    "    value=1,\n",
    "    data_type=\"NUMERIC\",\n",
    "    comment=\"This was correct, thank you\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2043ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab-rag-e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
